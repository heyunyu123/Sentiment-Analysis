{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS1801 Project: Sentiment Analysis\n",
    "\n",
    "## Part 2: Word Embeddings and Neural Network\n",
    "\n",
    "**Outline**:\n",
    "\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Neural Network\n",
    "- Word Embedding\n",
    "\n",
    "*Some codes are adapted from [deeplearning.ai](https://www.deeplearning.ai/). Please do not use the code for ANY commercial use.*\n",
    "\n",
    "** Pipeline **\n",
    "\n",
    "<img src=\"pipeline.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('3liCbRZPrZA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tf vectors we got last week\n",
    "with open('tf_vec.pkl', 'rb') as fp:\n",
    "    tf_vec = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data['reviewText'].tolist(), data['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# train_x, train_y = load_data('data/pptrain.csv')\n",
    "# test_x, test_y = load_data('data/pptest.csv')\n",
    "\n",
    "train_x, train_y = load_data('amazon_train.csv')\n",
    "test_x, test_y = load_data('amazon_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to vectors\n",
    "train_features = tf_vec.transform(train_x)\n",
    "test_features = tf_vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = LinearSVC()\n",
    "svm_model.fit(train_features, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(pred,test_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true=test_y, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = svm_model.decision_function(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = [1 if y == 'pos' else 0 for y in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = metrics.precision_recall_curve(y_test_bin, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Draw PR Curve\n",
    "plt.step(recall, precision, color='b', alpha=0.5, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.1, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve')\n",
    "\n",
    "# Add F-score to the plot as well\n",
    "f_scores = np.linspace(0.3, 0.9, num=5)\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "# Set the limit of the figure\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Note:**</span>\n",
    "\n",
    "For Simple Negation Handling: [Fast and accurate sentiment classification using an enhanced Naive Bayes model](https://pdfs.semanticscholar.org/c7a6/53d57e7f6686a13a37463f5e04e2916a5170.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$\n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "- Calculate $A = \\hat{Y} = \\sigma(w^T X + b)$\n",
    "- Calculate `cost`: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "### Back Propagation:\n",
    "\n",
    "- Calculate grad `dw`: $\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$\n",
    "- Calculate grad `db`: $\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$\n",
    "\n",
    "> `sigmoid` is already provided in **ml_utils.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte cost and grad (dw, db)\n",
    "# Put dw, db in a dict grad: {'dw': dw, 'db': db}\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    # Get number of training examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Get activation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    # Compute cost\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "    \n",
    "    # Compute grad\n",
    "    dw = 1/m * np.dot(X, (A-Y).T)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize (Gradient Descent)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"resources/gd1.gif\" alt=\"https://giphy.com/gifs/gradient-6QlTwkigqg4yk\"></td>\n",
    "        <td><img src='resources/gd2.gif' alt=\"http://songhuiming.github.io/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward & Backward prop\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    params = {'w': w, 'b': b}\n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_predict(w, b, X):\n",
    "    A = sigmoid(np.dot(w.T, X) + b) # np.array (1, X.shape[1])\n",
    "    return (A[0, :] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5):\n",
    "    \n",
    "    # Initialize w, b\n",
    "    w, b = np.zeros((X_train.shape[0], 1)), 0\n",
    "    \n",
    "    # Optimize\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # Test on training set and test set\n",
    "    Y_prediction_test = lr_predict(w, b, X_test)\n",
    "    Y_prediction_train = lr_predict(w, b, X_train)\n",
    "    \n",
    "    d = {\n",
    "        'costs': costs,\n",
    "        'Y_prediction_test': Y_prediction_test,\n",
    "        'Y_prediction_train': Y_prediction_train,\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "    }\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, X_test, Y_train, Y_test = load_iris()\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(X_train[0,:], X_train[1, :], c=Y_train[0], cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train, Y_train, X_test, Y_test, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_report(d, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (1-layer neural network)\n",
    "\n",
    "<img src=\"resources/1-layer-nn.png\">\n",
    "\n",
    "<br>\n",
    "<center>A 1-layer neural network: Logistic Regression</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_moon()\n",
    "plt.scatter(X_train[0,:], X_train[1,:],c=Y_train[0],cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(X_train, Y_train, X_test, Y_test, learning_rate=0.005, num_iterations=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_report(d, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: lr_predict(d['w'], d['b'], x.T), X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Notes:**</span>\n",
    "\n",
    "Logitic regression works not so well when features are not linearly separable. It depends heavily on features, so feature engineering is essential if you are using LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 2 - Layer Neural Network\n",
    "\n",
    "<img src=\"resources/2-layer-nn.png\">\n",
    "\n",
    "<br>\n",
    "<center>A 2-layer neural network: 1 hidden layer + 1 output layer</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Start of your code\n",
    "    # ------------------\n",
    "    # Calculate Z1, A1, Z2, A2\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # ------------------\n",
    "    # End of your code\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate `cost`: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    m = Y.shape[1]\n",
    "    cost = -1/m * np.sum(np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Start of your code\n",
    "    # ------------------\n",
    "    # Calculate dW1, dW2, db1, db2\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True) \n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1-np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # ------------------\n",
    "    # End of your code\n",
    "    \n",
    "    grads = {\n",
    "        'dW1': dW1,\n",
    "        'dW2': dW2,\n",
    "        'db1': db1,\n",
    "        'db2': db2,\n",
    "    }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.0):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    dW2 = grads['dW2']\n",
    "    db1 = grads['db1']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations=10000):\n",
    "    n_x, n_y = X.shape[0], Y.shape[0]\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Start of your code\n",
    "        # ------------------\n",
    "        \n",
    "        # Step 1: forward propagation\n",
    "        A2, cache =  forward_propagation(X, parameters)\n",
    "        \n",
    "        # Step 2: compute cost\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        \n",
    "        # Step 3: compute grad\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # Step 4: update parameters (grdient descent)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "\n",
    "        # ------------------\n",
    "        # End of your code\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return (A2 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X_train, Y_train, n_h = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: nn_predict(parameters, x.T), X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/word-vector.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_emoji()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_test.shape , Y_train.shape, Y_test.shape, X_train.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 samples\n",
    "for i in range(5):\n",
    "    print(X_train[i], label_to_emoji(Y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train[0], \"is converted into one hot\", Y_oh_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/word_embedding_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \n",
    "    # Start of your code\n",
    "    # ------------------\n",
    "    # Convert a sentence string into the average of word vector (dim = 50)\n",
    "    \n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    avg = np.zeros((50,))\n",
    "    cnt = 0\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map.get(w, np.zeros((50,)))\n",
    "        cnt += 1\n",
    "    avg = avg / cnt\n",
    "    \n",
    "    # ------------------\n",
    "    # End of your code\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = sentence_to_avg(\"I like it\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Assuming here that $Yoh$ (\"Y one hot\") is the one-hot encoding of the output labels, the equations you need to implement in the forward pass and to compute the cross-entropy cost are:\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
    "\n",
    "> `softmax()` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            \n",
    "            # Start of your code\n",
    "            # ------------------\n",
    "            # Convert a sentence string into the average of word vector (dim = 50)\n",
    "            \n",
    "            # Step 0: sentence to average vector\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            \n",
    "            # Step 1: forward propagation\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "            \n",
    "            # Step 2: compute cost\n",
    "            cost = -np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            # Step 3: compute grad\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            # Step 4: update parameters (grdient descent)\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            # ------------------\n",
    "            # End of your code\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\n",
    "    \"you romanced me\", \n",
    "    \"you are horrible\", \n",
    "    \"funny lol\", \n",
    "    \"lets play with a ball\", \n",
    "    \"food is ready\", \n",
    "    \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [3], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
